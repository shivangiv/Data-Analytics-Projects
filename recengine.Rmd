---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

<style>
body {
text-align: justify}
</style>


$\large 4. Recommendation System $ We attempt to create a recommendation system that will recommend books given a user based on collaborative filtering method.


 
## Data Exploration and pre processing

I used the goodreads data set which only had meta data about books, to see whether those could help me in making the recommending system. However, I quickly found out that these variables will not help me and don't significantly influence ratings of a book and won't help in making the recommendation engine.
At some point, trying to make a model with that dataset did not work. 

I also looked at how netflix does its recommending, and discovered it uses a technique called collaborative filtering (Netflix Recommendation Engine, n.d.).
In this model, a probability model is used that scores the relationship between the user and movie type to predict preferences.
For this reason I downloaded more goodreads data which has user ratings as well.
This(Foxtrot, 2018) dataset has over 53.4k user ratings, books, genre and other book related information.


```{r}
library(data.table)
library(tidyverse)

books10k<-fread("goodbooks-10k/books.csv")
ratings10k<-fread("goodbooks-10k/ratings.csv")
booktags10k<-fread("goodbooks-10k/book_tags.csv")
tags10k<-fread("goodbooks-10k/tags.csv")
head(books10k)
head(ratings10k)
head(tags10k)
```

### Pre processing to clean data.

Data contains duplicates that need to be removed, ie same user has rated the same book multiple times. I also removed records of users that have only rated less than 3 books, since their ratings are insignificant. Their information will not influence the recommendation system.

Since this data set is large, I chose to use only 40% of the user data to create the system.
```{r echo=FALSE}
# Removing duplicate ratings
ratings10k[, N := .N, .(user_id, book_id)]
ratings10k <- ratings10k[N == 1]

ratings10k[!duplicated(ratings10k),]
ratings10k<-as.data.table(ratings10k)
ratings10k[, N := .N, .(user_id)]

#remove users who rated less than 3 books
ratings10k <- ratings10k[N > 2]

# selecting 40% of users to create the recc system from
set.seed(1)
sample_size <- 0.4
users <- unique(ratings10k$user_id)
sample_users <- sample(users, round(sample_size * length(users)))
sprintf('Number of ratings (before): %d ', nrow(ratings10k))
ratings10k <- ratings10k[ratings10k$user_id %in% sample_users]
sprintf('Number of ratings (after): %d ', nrow(ratings10k))


```

### Exploration

Distribution of ratings, mean user ratings, etc were explored. 
Those that rate frequently, do they rate differently from less frequent users ? We can explore this by seeing the next plot.
Results:
The plots showed books are rated 4-5. I saw how users rate, ie number of ratings per user. Then, the ones that rate more, how differently do they rate? I found that frequent users give more lower ratings, could either be they critique more as they read, etc.
```{r}
library(grid)
library(gridExtra)
library(corrplot)
library(methods)
library(Matrix)

#how are ratings distributed
ratings10k %>% 
  ggplot(aes(x = rating, fill = factor(rating))) +
  geom_bar(color = "grey20")

# No of ratings per user
ratings10k %>% 
  group_by(user_id) %>% 
  summarize(number_of_ratings_per_user = n()) %>% 
  ggplot(aes(number_of_ratings_per_user)) + 
  geom_bar() + coord_cartesian(c(3, 50))

# view how users rate and their frequency
get_cor <- function(df){
    m <- cor(df$x,df$y, use="pairwise.complete.obs");
    eq <- substitute(italic(r) == cor, list(cor = format(m, digits = 2)))
    as.character(as.expression(eq));                 
}

#How do users rate according to their frequency?
tmp <- ratings10k %>% 
  group_by(user_id) %>% 
  summarize(mean_rating = mean(rating), number_of_rated_books = n())

tmp %>% filter(number_of_rated_books <= 100) %>% 
  ggplot(aes(number_of_rated_books, mean_rating)) + stat_bin_hex(bins = 50) + scale_fill_distiller() + stat_smooth(method = "lm", size = 2, se = FALSE) +
  annotate("text", x = 80, y = 1.9, label = get_cor(data.frame(x = tmp$number_of_rated_books, y = tmp$mean_rating)), size = 7, parse = TRUE)

```


#### How are genres distributed?

Since there are user assigned tags and genres, I am choosing those tags that match good-reads built in genres. Since, looking at the tags, some tags do not make sense like some tags are numeric that do not make sense and will not help in recommending.

Therefore I made a list of most common genre tags and then removed some generic genres like fiction, non fiction, ebooks, etc.

I then looked at how genres are distributed.
```{r}
#selecting only goodreads genre tags
genres <- str_to_lower(c("Art", "Biography", "Business", "Chick Lit", "Children's", "Christian", "Classics", "Comics", "Contemporary", "Cookbooks", "Crime", "Ebooks", "Fantasy", "Fiction", "Gay and Lesbian", "Graphic Novels", "Historical Fiction", "History", "Horror", "Humor and Comedy", "Manga", "Memoir", "Music", "Mystery", "Nonfiction", "Paranormal", "Philosophy", "Poetry", "Psychology", "Religion", "Romance", "Science", "Science Fiction", "Self Help", "Suspense", "Spirituality", "Sports", "Thriller", "Travel", "Young Adult"))

exclude_genres <- c("fiction", "nonfiction", "ebooks", "contemporary")
genres <- setdiff(genres, exclude_genres)

available_genres <- genres[str_to_lower(genres) %in% tags10k$tag_name]
available_tags <- tags10k$tag_id[match(available_genres, tags10k$tag_name)]

#Distribution of genres
genredist <- booktags10k %>% 
  filter(tag_id %in% available_tags) %>% 
  group_by(tag_id) %>%
  summarize(n = n()) %>%
  ungroup() %>%
  mutate(sumN = sum(n), percentage = n / sumN) %>%
  arrange(-percentage) %>%
  left_join(tags10k, by = "tag_id")

genredist %>% 
  ggplot(aes(reorder(tag_name, percentage), percentage, fill = percentage)) + geom_bar(stat = "identity") + coord_flip() + scale_fill_distiller() + labs(y = 'Percentage', x = 'Genre')
```
This shows that most books fall under the fantasy, romance and mystery genre.
When a user first joins Netflix and has not rated any shows or movies so far, Netflix just recommends the most popular or trending shows/ movies. A similar thing could be done looking at this genre information.

#### What influences a book's rating?

Correlation plot reveals no strong relations between rating and rating count, text review count etc. Hence other factors might affect it.
```{r}
library(ggcorrplot)

cormatrix <- books10k %>% 
  select(one_of(c("books_count","original_publication_year","ratings_count", "work_ratings_count", "work_text_reviews_count", "average_rating"))) %>% 
  as.matrix()

ggcorrplot(cor(cormatrix, use = 'pairwise.complete.obs'), type = "lower")

```

### Collaborative filtering

From the EDA it looks like effects of meta data type variables like ratings count is insignificant, and other quality aspects matter more.

We use the aforementioned user-based collaborative filtering method to make recommendations. It is a popular technique used by Netflix for recommending movies. The way it works is very intuitivw; when thinking of what book to read next, you are most likely going to ask that one friend who has similar taste as you to recommend some to you.
Hence, similarly, we will first find similar users to a chosen current user in terms of their ratings on the same set of books. Then, we will find avg ratings for books rated by those similar users that the current user has not yet read. In theory, the highest avg rated books can be good recommendations to that chosen user.

#### Pre-processing before creating recommendation engine

We need to create a sort of utility matrix on books rated by users so that we know which users have rated which books.
```{r}

dimension_names <- list(user_id = sort(unique(ratings10k$user_id)), book_id = sort(unique(ratings10k$book_id)))

ratingmat <- spread(select(ratings10k, book_id, user_id, rating), book_id, rating) %>% select(-user_id)

ratingmat <- as.matrix(ratingmat)
dimnames(ratingmat) <- dimension_names
ratingmat[1:5, 1:5]
rt<-as.data.frame(ratingmat)


```


#### Find similar users

Lets select a random user id 173. We find users that rated atleast one book rated by this user, lets call the user Bob.
For this user, there are over 700 similar users.
We find similarity with Bob's ratings. We can use Pearson's correlation coefficient. We sort these according to highest similarity. we now have most similar users.


```{r}
library(igraph)
chosen_user<-"173"

rated_items <- which(!is.na((as.data.frame(ratingmat[chosen_user, ]))))
selected_users <- names(which(apply(!is.na(ratingmat[ ,rated_items]), 1, sum) >= 2))
head(selected_users,40)


#Normalizing the ratings so as to equalise variance 
rmat<-scale(ratingmat[selected_users, ])

# Finding similarity and sorting acc to highest similarity
similarities <- cor(t(rmat[rownames(rmat)!=chosen_user, ]), rmat[chosen_user, ], use = 'pairwise.complete.obs')
sim <- as.vector(similarities)
names(sim) <- rownames(similarities)
res <- sort(sim, decreasing = TRUE)
head(res, 40)
```


I created a network graph to visualize this similarity with 20 users.
```{r}
#Visualizing similarity
random_users <- selected_users[1:20]
sim_mat <- cor(t(rmat), use = 'pairwise.complete.obs')

g<-graph_from_adjacency_matrix(sim_mat[c(chosen_user, random_users), c(chosen_user, random_users)], mode="undirected", diag=FALSE)
plot(g)


```

### Getting predictions for other books

We take most similar books and avg their ratings for books Bob has not yet rated. We only include those books that have been rated by many other users
```{r}

similar_users <- names(res[1:4])

similar_users_ratings <- data.frame(item = rep(colnames(rmat), length(similar_users)), rating = c(t(as.data.frame(rmat[similar_users,])))) %>% filter(!is.na(rating))

chosen_user_ratings <- data.frame(item = colnames(rmat), rating = rmat[chosen_user,]) %>% filter(!is.na(rating))

predictions <- similar_users_ratings %>% 
  filter(!(item %in% chosen_user_ratings$item)) %>% 
  group_by(item) %>% summarize(mean_rating = mean(rating))


#Recommend best 5 predictions
bookrecs<-predictions %>% 
  arrange(-mean_rating) %>% 
  top_n(5, wt = mean_rating) %>% 
  mutate(book_id = as.numeric(as.character(item))) %>% 
  left_join(select(books10k, authors, title, id), by = c("book_id" = "id")) %>% 
  select(-item) 

bookrecs

```

### Using recommender lab

Recommender Lab is a powerful R package solely used for recommendation systems. It has many collaborative filtering based algorithms and uses clustering as well. It also has inbuilt cross validation function to help us test the model.
We will be using recommender lab to generate 5 books and compare the results with the results found using our own collaborative filtering algorithm above.

```{r message=FALSE, warning=FALSE}
library(recommenderlab)
ratingmat0 <- ratingmat
ratingmat0[is.na(ratingmat0)] <- 0
sparse_ratings <- as(ratingmat0, "sparseMatrix")
rm(ratingmat0)
gc()

real_ratings <- new("realRatingMatrix", data = sparse_ratings)
real_ratings

model <- Recommender(real_ratings, method = "UBCF", param = list(method = "pearson", nn = 4))

#Making predictions 
prediction <- predict(model, real_ratings[chosen_user, ], type = "ratings")

reclabresult<-as(prediction, 'data.frame') %>% 
  arrange(-rating) %>% .[1:5,] %>% 
  mutate(book_id = as.numeric(as.character(item))) %>% 
  left_join(select(books10k, authors, title, id), by = c("book_id" = "id")) %>% 
  select(-item)
reclabresult
```

As you can see, using clustering algorithm, the results seem more accurate, because the ratings of these books are higher than the ratings  of books found via collaborative filtering.


### Comparing performance when number of neighbours are increased

```{r}

scheme <- evaluationScheme(real_ratings[1:500,], method = "cross-validation", k = 10, given = -1, goodRating = 5)

algorithms <- list(
                   "UBCF_05" = list(name = "UBCF", param = list(nn = 5)),
                   "UBCF_10" = list(name = "UBCF", param = list(nn = 10)),
                   "UBCF_30" = list(name = "UBCF", param = list(nn = 30)),
                   "UBCF_50" = list(name = "UBCF", param = list(nn = 50))
                   )

results <- evaluate(scheme, algorithms, type = "ratings")

tmp <- lapply(results, function(x) slot(x, "results"))
res <- tmp %>% 
  lapply(function(x) unlist(lapply(x, function(x) unlist(x@cm[ ,"RMSE"])))) %>% 
  as.data.frame() %>% 
  gather(key = "Algorithm", value = "RMSE")

rmse<-aggregate(res,by=list(res$Algorithm),FUN = mean)
rmse

```

$\large Comparing~the~models:$  <br>

We see that recommenderLab's algorithm works slightly better, in that the books it outputs have a better mean rating than the collaborative filter we created. We can also compare the RMSE of the algorithm when the number of nearest neighbours is increased.
As you can see, when the number increases, the RMSE gets better. That means if you take into consideration more similar users, your recommendation will be better.

$\large Conclusion:$  <br>

We have learnt several methods of data analysis implementation in this class. With our simple project, we learnt how to do data exploration, which is a key process before building any sort of model. This project uses descriptive statistics to first analyse the data and distribution of its variables. We then built a recommendation system using two methods- a collaborative filter designed over finding similar users and recommending books they have read, but which the current user has not, and then using recommenderLab's built in UBCF (user based collaborative filtering) algorithm and seeing how both compare. When the top-k nearset neighbours are chosen we get the best result as seen.

$\large References:$ </br>  
goodbooks-10k, Foxtrot, 2018. Ten thousand books, one million ratings. Also books marked to read, and tags. Retrieved from kaggle.com:https://www.kaggle.com/zygmunt/goodbooks-10k

Netflix Recommendation Engine. Retrieved from codeacademy.com:https://www.codecademy.com/articles/how-netflix-recommendation-works-data-science

Soumik. (2020, March 9). Goodreads-books. Retrieved from https://www.kaggle.com/jealousleopard/goodreadsbooks/kernels

What Makes a Book Worth Reading? (2018, May 1). Retrieved from http://booksmakeadifference.com/bookgood/

